import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.distributed import DistributedSampler
from diffusers import AudioLDM2Pipeline
from diffusers import AutoencoderKL
import torchaudio
import librosa
import os
import numpy as np
from tqdm import tqdm
import traceback
from audioldm2.utils import default_audioldm_config
from audioldm2.utilities.audio.stft import TacotronSTFT
from diffusers import AutoencoderKL

# ========== ä¿®æ”¹çš„éŸ³é¢‘å¤„ç†å‡½æ•°ä»¥æ”¯æŒæ‰¹å¤„ç† ==========

def get_mel_from_wav_batch(audio_batch, _stft):
    """å¤„ç†ä¸€æ‰¹éŸ³é¢‘"""
    # audio_batch: [batch_size, samples] æˆ– [batch_size, 1, samples]
    # if audio_batch.dim() == 2:
    #     audio_batch = audio_batch.unsqueeze(1)  # [batch_size, 1, samples]
    # elif audio_batch.dim() == 3 and audio_batch.size(1) == 1:
    #     audio_batch = audio_batch.squeeze(1)  # ç¡®ä¿æ˜¯ [batch_size, samples]
    # print(audio_batch.shape)
    
    audio_batch = torch.clip(audio_batch, -1, 1)
    audio_batch = torch.autograd.Variable(audio_batch, requires_grad=False)
    
    # æ‰¹é‡å¤„ç†
    melspec, magnitudes, phases, energy = _stft.mel_spectrogram(audio_batch)
    
    return melspec, magnitudes, energy

def pad_wav(waveform, segment_length):
    waveform_length = waveform.shape[-1]
    assert waveform_length > 100, "Waveform is too short, %s" % waveform_length
    if segment_length is None or waveform_length == segment_length:
        return waveform
    elif waveform_length > segment_length:
        return waveform[:segment_length]
    elif waveform_length < segment_length:
        temp_wav = np.zeros((1, segment_length))
        temp_wav[:, :waveform_length] = waveform
    return temp_wav

def _pad_spec(fbank, target_length=1024):
    n_frames = fbank.shape[0]
    p = target_length - n_frames
    # cut and pad
    if p > 0:
        m = torch.nn.ZeroPad2d((0, 0, 0, p))
        fbank = m(fbank)
    elif p < 0:
        fbank = fbank[0:target_length, :]

    if fbank.size(-1) % 2 != 0:
        fbank = fbank[..., :-1]

    return fbank

def normalize_wav(waveform):
    waveform = waveform - np.mean(waveform)
    waveform = waveform / (np.max(np.abs(waveform)) + 1e-8)
    return waveform * 0.5

def read_wav_file(filename, segment_length):
    try:
        waveform, sr = torchaudio.load(filename, format="mp4", backend="ffmpeg")
        waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=16000)
        waveform = waveform.numpy()[0, ...]
        waveform = normalize_wav(waveform)
        waveform = waveform[None, ...]
        waveform = pad_wav(waveform, segment_length)

        max_val = np.max(np.abs(waveform))
        if max_val > 1e-8:
            waveform = waveform / max_val
            waveform = 0.5 * waveform
        return waveform
    except Exception as e:
        print(f"Error reading {filename}: {e}")
        return None

def wav_to_fbank_batch(filenames, target_length=1024, fn_STFT=None, device=None):
    """æ‰¹é‡å¤„ç†å¤šä¸ªéŸ³é¢‘æ–‡ä»¶"""
    assert fn_STFT is not None
    
    batch_waveforms = []
    valid_indices = []
    
    # è¯»å–æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
    for i, filename in enumerate(filenames):
        waveform = read_wav_file(filename, target_length * 160)
        if waveform is not None:
            waveform = waveform[0, ...]
            batch_waveforms.append(waveform)
            valid_indices.append(i)
    
    if not batch_waveforms:
        return None, None, None, []
    
    # è½¬æ¢ä¸ºæ‰¹é‡tensor
    batch_waveforms = torch.FloatTensor(np.stack(batch_waveforms)).to(device)
    
    # æ‰¹é‡å¤„ç†mel spectrogram
    fbank_batch, log_magnitudes_stft_batch, energy_batch = get_mel_from_wav_batch(
        batch_waveforms, fn_STFT
    )
    
    # å¤„ç†æ¯ä¸ªæ ·æœ¬çš„padding
    processed_fbanks = []
    processed_log_mags = []
    
    for i in range(fbank_batch.size(0)):
        fbank = fbank_batch[i].T
        log_mag = log_magnitudes_stft_batch[i].T
        
        fbank = _pad_spec(fbank, target_length)
        log_mag = _pad_spec(log_mag, target_length)
        
        processed_fbanks.append(fbank)
        processed_log_mags.append(log_mag)
    
    # Stackå›batch
    fbank_batch = torch.stack(processed_fbanks)
    log_magnitudes_stft_batch = torch.stack(processed_log_mags)
    
    return fbank_batch, log_magnitudes_stft_batch, batch_waveforms, valid_indices

def encode_audio_batch_from_videos(video_paths, vae, fn_STFT, device, batch_size=8):
    """æ‰¹é‡ç¼–ç å¤šä¸ªè§†é¢‘çš„éŸ³é¢‘"""
    try:
        config = default_audioldm_config()
        duration = 3
        
        # æ‰¹é‡å¤„ç†éŸ³é¢‘
        mel_batch, _, _, valid_indices = wav_to_fbank_batch(
            video_paths, 
            target_length=int(duration * 102.4), 
            fn_STFT=fn_STFT, 
            device=device
        )
        
        if mel_batch is None:
            return [None] * len(video_paths), ["FAILED"] * len(video_paths)
        
        # mel_batch: [batch_size, freq, time]
        mel_batch = mel_batch.unsqueeze(1).to(torch.float16)  # [batch_size, 1, freq, time]
        
        # æ‰¹é‡ç¼–ç 
        with torch.no_grad():
            latent_representations = vae.encode(mel_batch).latent_dist.mode()
        
        # å‡†å¤‡ç»“æœ
        results = [None] * len(video_paths)
        statuses = ["FAILED"] * len(video_paths)
        
        # å¡«å……æˆåŠŸçš„ç»“æœ
        for i, valid_idx in enumerate(valid_indices):
            results[valid_idx] = latent_representations[i].cpu().numpy()
            statuses[valid_idx] = "SUCCESS"
        
        return results, statuses
        
    except Exception as e:
        error_message = f"æ‰¹å¤„ç†æ—¶å‘ç”Ÿé”™è¯¯: {e}\n{traceback.format_exc()}"
        print(error_message)
        return [None] * len(video_paths), ["FAILED"] * len(video_paths)

# ========== ä¿®æ”¹çš„Datasetç±»ä»¥æ”¯æŒæ‰¹å¤„ç† ==========

class VideoDataset(Dataset):
    """ç®€å•çš„æ•°æ®é›†ç±»ç”¨äºåŠ è½½è§†é¢‘æ–‡ä»¶è·¯å¾„"""
    def __init__(self, input_dir, output_dir):
        self.input_dir = input_dir
        self.output_dir = output_dir
        self.files = sorted([f for f in os.listdir(input_dir) if f.lower().endswith(".mp4")])
        
    def __len__(self):
        return len(self.files)
    
    def __getitem__(self, idx):
        filename = self.files[idx]
        video_path = os.path.join(self.input_dir, filename)
        base_name = os.path.splitext(filename)[0]
        output_path = os.path.join(self.output_dir, f"{base_name}.npy")
        return video_path, output_path

def collate_fn(batch):
    """è‡ªå®šä¹‰collateå‡½æ•°æ¥å¤„ç†æ‰¹é‡æ•°æ®"""
    video_paths = [item[0] for item in batch]
    output_paths = [item[1] for item in batch]
    return video_paths, output_paths

def setup_ddp(rank, world_size):
    """åˆå§‹åŒ–DDPç¯å¢ƒ"""
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def cleanup():
    """æ¸…ç†DDP"""
    dist.destroy_process_group()

def setup_audioldm2_vae_ddp(rank, repo_id="cvssp/audioldm2", torch_dtype=torch.float16):
    """ä¸ºDDPåŠ è½½å¹¶è®¾ç½®AudioLDM 2 VAEæ¨¡å‹"""
    device = torch.device(f"cuda:{rank}")
    
    if rank == 0:
        print(f"[Rank {rank}]: æ­£åœ¨åŠ è½½ AudioLDM 2 æ¨¡å‹...")
    vae = AutoencoderKL.from_pretrained(
        repo_id, subfolder="vae", torch_dtype=torch_dtype, resume_download=True
    )
    vae = vae.to(device)
    vae.eval()
    for p in vae.parameters():
        p.requires_grad = False
    
    if rank == 0:
        print(f"[Rank {rank}]: æ¨¡å‹å·²æˆåŠŸåŠ è½½ã€‚")
    
    return vae, None, device

def process_batch_ddp(rank, world_size, input_dir, output_dir, batch_size=8):
    """DDPå·¥ä½œå‡½æ•°ï¼Œå¤„ç†åˆ†é…ç»™å½“å‰rankçš„æ•°æ®"""
    # è®¾ç½®DDP
    setup_ddp(rank, world_size)
    
    try:
        # åŠ è½½æ¨¡å‹
        vae, _, device = setup_audioldm2_vae_ddp(rank)
        
        # åˆ›å»ºæ•°æ®é›†å’Œåˆ†å¸ƒå¼é‡‡æ ·å™¨
        dataset = VideoDataset(input_dir, output_dir)
        sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=False)
        
        # ä½¿ç”¨æ›´å¤§çš„batch sizeå’Œè‡ªå®šä¹‰collateå‡½æ•°
        dataloader = DataLoader(
            dataset, 
            batch_size=batch_size, 
            sampler=sampler, 
            num_workers=2,  # å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹
            collate_fn=collate_fn,
            pin_memory=True  # åŠ é€Ÿæ•°æ®ä¼ è¾“åˆ°GPU
        )
        
        # è®¾ç½®STFT
        config = default_audioldm_config()
        fn_STFT = TacotronSTFT(
            config["preprocessing"]["stft"]["filter_length"],
            config["preprocessing"]["stft"]["hop_length"],
            config["preprocessing"]["stft"]["win_length"],
            config["preprocessing"]["mel"]["n_mel_channels"],
            config["preprocessing"]["audio"]["sampling_rate"],
            config["preprocessing"]["mel"]["mel_fmin"],
            config["preprocessing"]["mel"]["mel_fmax"],
        ).to(device)
        
        error_count = 0
        success_count = 0
        
        # è®¡ç®—æ€»æ‰¹æ¬¡æ•°
        total_batches = len(dataloader)
        
        # åªåœ¨rank 0æ˜¾ç¤ºè¿›åº¦æ¡
        iterator = tqdm(dataloader, desc=f"Rank {rank} å¤„ç†ä¸­ (batch_size={batch_size})") if rank == 0 else dataloader
        
        for batch_idx, (video_paths, output_paths) in enumerate(iterator):
            # æ‰¹é‡ç¼–ç 
            latent_results, statuses = encode_audio_batch_from_videos(
                video_paths, vae, fn_STFT, device, batch_size
            )
            
            # ä¿å­˜ç»“æœ
            for i, (latent_np, status, output_path) in enumerate(zip(latent_results, statuses, output_paths)):
                if status == "SUCCESS" and latent_np is not None:
                    np.save(output_path, latent_np)
                    success_count += 1
                else:
                    if rank == 0:
                        print(f"[Rank {rank} é”™è¯¯]: æ–‡ä»¶ {os.path.basename(video_paths[i])} å¤„ç†å¤±è´¥")
                    error_count += 1
            
            # å®šæœŸæ¸…ç†ç¼“å­˜
            if batch_idx % 10 == 0:
                torch.cuda.empty_cache()
        
        # æ”¶é›†æ‰€æœ‰rankçš„ç»Ÿè®¡ä¿¡æ¯
        error_tensor = torch.tensor([error_count], device=device)
        success_tensor = torch.tensor([success_count], device=device)
        
        dist.all_reduce(error_tensor, op=dist.ReduceOp.SUM)
        dist.all_reduce(success_tensor, op=dist.ReduceOp.SUM)
        
        if rank == 0:
            print(f"\næ€»è®¡æˆåŠŸå¤„ç†: {success_tensor.item()} ä¸ªæ–‡ä»¶")
            print(f"æ€»è®¡å¤„ç†å¤±è´¥: {error_tensor.item()} ä¸ªæ–‡ä»¶")
            
    except Exception as e:
        print(f"[Rank {rank}]: å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        traceback.print_exc()
    finally:
        if 'vae' in locals():
            del vae
        torch.cuda.empty_cache()
        cleanup()

def batch_process_videos_ddp(input_dir, output_dir, batch_size=8):
    """ä¸»å‡½æ•°ï¼šä½¿ç”¨DDPå¤„ç†è§†é¢‘"""
    # æ£€æŸ¥GPUæ•°é‡
    if not torch.cuda.is_available():
        print("é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°CUDAè®¾å¤‡ã€‚")
        return
    
    world_size = torch.cuda.device_count()
    print(f"æ£€æµ‹åˆ° {world_size} å—å¯ç”¨çš„GPUï¼Œå°†ä½¿ç”¨DDPè¿›è¡Œå¤„ç†ã€‚")
    print(f"æ¯ä¸ªGPUçš„batch size: {batch_size}")
    
    # æ£€æŸ¥å¹¶åˆ›å»ºè¾“å‡ºç›®å½•
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"å·²åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    all_files = [f for f in os.listdir(input_dir) if f.lower().endswith(".mp4")]
    if not all_files:
        print(f"åœ¨ç›®å½• '{input_dir}' ä¸­æ²¡æœ‰æ‰¾åˆ° .mp4 æ–‡ä»¶ã€‚")
        return
    
    print(f"åœ¨è¾“å…¥ç›®å½•ä¸­æ‰¾åˆ° {len(all_files)} ä¸ª .mp4 æ–‡ä»¶ã€‚")
    
    # ä½¿ç”¨spawnå¯åŠ¨DDPè¿›ç¨‹
    mp.spawn(
        process_batch_ddp,
        args=(world_size, input_dir, output_dir, batch_size),
        nprocs=world_size,
        join=True
    )
    torch.cuda.empty_cache()
    print("\nğŸ‰ æ‰€æœ‰è§†é¢‘å¤„ç†å®Œæˆï¼")

if __name__ == '__main__':
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    
    # å¯ä»¥æ ¹æ®GPUå†…å­˜è°ƒæ•´batch size
    BATCH_SIZE = 64  # ä»8å¼€å§‹ï¼Œå¦‚æœå†…å­˜å…è®¸å¯ä»¥å¢åŠ åˆ°16æˆ–32
    video_directory_list = ["vggsound_10_3s", "vggsound_11_3s", "vggsound_12_3s", "vggsound_13_3s", "vggsound_14_3s"]
    # video_directory_list = ["vggsound_15_3s", "vggsound_16_3s", "vggsound_17_3s", "vggsound_18_3s", "vggsound_19_3s"]
    input_video_directory_base = "/blob/vggsound_cropped"
    output_latent_directory_base = "/blob/vggsound_cropped_audio_latent_fixed"
   
    for video_dir in video_directory_list:
        print(f"\nå¤„ç†ç›®å½•: {video_dir}")
        input_video_directory = os.path.join(input_video_directory_base, video_dir)
        output_latent_directory = os.path.join(output_latent_directory_base, video_dir)
        
        try:
            batch_process_videos_ddp(input_video_directory, output_latent_directory, batch_size=BATCH_SIZE)
        except Exception as e:
            print(f"\nå¤„ç† {video_dir} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            print(traceback.format_exc())
