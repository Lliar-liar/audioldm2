import torch
import torchaudio
import numpy as np
from scipy.linalg import sqrtm
from scipy.stats import entropy
import warnings

# å®‰è£…ä¾èµ–ï¼ˆå¦‚æœéœ€è¦ï¼‰ï¼š
# pip install torchvision torchaudio

class FADCalculator:
    """è®¡ç®—FrÃ©chet Audio Distance (FAD)"""
    
    def __init__(self, model_name='vggish', device='cuda'):
        """
        åˆå§‹åŒ–FADè®¡ç®—å™¨
        model_name: 'vggish' æˆ– 'pann'
        """
        self.device = device
        self.model_name = model_name
        
        if model_name == 'vggish':
            self.model = self._load_vggish()
        elif model_name == 'pann':
            self.model = self._load_pann()
        else:
            raise ValueError(f"Unknown model: {model_name}")
    
    def _load_vggish(self):
        """åŠ è½½VGGishæ¨¡å‹ (Googleçš„éŸ³é¢‘ç‰¹å¾æå–å™¨)"""
        try:
            # æ–¹æ³•1: ä½¿ç”¨torchaudioçš„é¢„è®­ç»ƒæ¨¡å‹
            from torchaudio.models import vggish
            model = vggish()
            model.eval()
            model.to(self.device)
            print("âœ“ åŠ è½½äº†torchaudio VGGish")
            return model
        except:
            print("! VGGishä¸å¯ç”¨ï¼Œä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ...")
            return self._load_simple_extractor()
    
    def _load_pann(self):
        """åŠ è½½PANNs (æ›´ç°ä»£çš„éŸ³é¢‘ç‰¹å¾æå–å™¨)"""
        try:
            # éœ€è¦å®‰è£…: pip install panns-inference
            from panns_inference import AudioTagging
            model = AudioTagging(checkpoint_path=None, device=self.device)
            print("âœ“ åŠ è½½äº†PANNs")
            return model
        except:
            print("! PANNsä¸å¯ç”¨ï¼Œä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ...")
            return self._load_simple_extractor()
    
    def _load_simple_extractor(self):
        """ç®€å•çš„ç‰¹å¾æå–å™¨ä½œä¸ºåå¤‡æ–¹æ¡ˆ"""
        class SimpleAudioFeatureExtractor(torch.nn.Module):
            def __init__(self):
                super().__init__()
                # ä½¿ç”¨é¢„è®­ç»ƒçš„wav2vec2æˆ–å…¶ä»–å¯ç”¨æ¨¡å‹
                try:
                    from transformers import Wav2Vec2Model, Wav2Vec2Processor
                    self.processor = Wav2Vec2Processor.from_pretrained(
                        "facebook/wav2vec2-base"
                    )
                    self.model = Wav2Vec2Model.from_pretrained(
                        "facebook/wav2vec2-base"
                    )
                    self.model.eval()
                    print("âœ“ ä½¿ç”¨Wav2Vec2ä½œä¸ºç‰¹å¾æå–å™¨")
                except:
                    # æœ€åçš„åå¤‡ï¼šç®€å•çš„é¢‘è°±ç‰¹å¾
                    print("âœ“ ä½¿ç”¨é¢‘è°±ç‰¹å¾æå–å™¨")
                    self.processor = None
                    self.model = None
            
            def forward(self, audio, sr=16000):
                if self.model is not None:
                    inputs = self.processor(
                        audio, 
                        sampling_rate=sr, 
                        return_tensors="pt"
                    )
                    with torch.no_grad():
                        outputs = self.model(**inputs)
                    # ä½¿ç”¨æœ€åéšè—çŠ¶æ€çš„å¹³å‡ä½œä¸ºç‰¹å¾
                    features = outputs.last_hidden_state.mean(dim=1)
                else:
                    # ç®€å•çš„é¢‘è°±ç‰¹å¾
                    features = self._extract_spectral_features(audio, sr)
                return features
            
            def _extract_spectral_features(self, audio, sr):
                """æå–ç®€å•çš„é¢‘è°±ç»Ÿè®¡ç‰¹å¾"""
                # è®¡ç®—melé¢‘è°±å›¾
                mel_spec = torchaudio.transforms.MelSpectrogram(
                    sample_rate=sr,
                    n_fft=2048,
                    hop_length=512,
                    n_mels=128
                )(torch.tensor(audio).float())
                
                # æå–ç»Ÿè®¡ç‰¹å¾
                features = []
                features.append(mel_spec.mean(dim=-1))  # æ—¶é—´å¹³å‡
                features.append(mel_spec.std(dim=-1))   # æ—¶é—´æ ‡å‡†å·®
                features.append(mel_spec.max(dim=-1)[0]) # æœ€å¤§å€¼
                features.append(mel_spec.min(dim=-1)[0]) # æœ€å°å€¼
                
                # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
                return torch.cat(features, dim=-1)
        
        return SimpleAudioFeatureExtractor().to(self.device)
    
    def extract_features(self, audio, sr=16000):
        """
        æå–éŸ³é¢‘ç‰¹å¾
        audio: numpy array æˆ– torch tensor
        sr: é‡‡æ ·ç‡
        """
        if isinstance(audio, np.ndarray):
            audio = torch.tensor(audio).float()
        
        if audio.dim() == 1:
            audio = audio.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        
        audio = audio.to(self.device)
        
        with torch.no_grad():
            if self.model_name == 'vggish':
                # VGGishéœ€è¦ç‰¹å®šçš„é¢„å¤„ç†
                if hasattr(self.model, 'forward'):
                    features = self.model(audio, sr)
                else:
                    features = self.model(audio)
            elif self.model_name == 'pann':
                features = self.model.inference(audio.cpu().numpy())[1]
                features = torch.tensor(features).to(self.device)
            else:
                features = self.model(audio.cpu().numpy(), sr)
        
        return features.cpu().numpy()
    
    def calculate_statistics(self, features):
        """è®¡ç®—ç‰¹å¾çš„å‡å€¼å’Œåæ–¹å·®"""
        if len(features.shape) == 1:
            features = features.reshape(1, -1)
        
        mu = np.mean(features, axis=0)
        sigma = np.cov(features, rowvar=False)
        
        return mu, sigma
    
    def calculate_fad(self, features1, features2):
        """
        è®¡ç®—ä¸¤ç»„ç‰¹å¾ä¹‹é—´çš„FrÃ©chetè·ç¦»
        è¿™æ˜¯FADçš„æ ¸å¿ƒè®¡ç®—
        """
        mu1, sigma1 = self.calculate_statistics(features1)
        mu2, sigma2 = self.calculate_statistics(features2)
        
        # è®¡ç®—FrÃ©chetè·ç¦»
        diff = mu1 - mu2
        
        # å¤„ç†åæ–¹å·®çŸ©é˜µ
        if len(sigma1.shape) == 0:  # æ ‡é‡æƒ…å†µ
            sigma1 = np.array([[sigma1]])
            sigma2 = np.array([[sigma2]])
        elif len(sigma1.shape) == 1:  # ä¸€ç»´æƒ…å†µ
            sigma1 = np.diag(sigma1)
            sigma2 = np.diag(sigma2)
        
        # è®¡ç®—åæ–¹å·®ä¹˜ç§¯çš„å¹³æ–¹æ ¹
        covmean, _ = sqrtm(sigma1.dot(sigma2), disp=False)
        
        # æ•°å€¼ç¨³å®šæ€§ï¼šç¡®ä¿å®æ•°
        if np.iscomplexobj(covmean):
            if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):
                m = np.max(np.abs(covmean.imag))
                warnings.warn(f"å¤æ•°åæ–¹å·®çŸ©é˜µï¼Œè™šéƒ¨æœ€å¤§å€¼: {m}")
            covmean = covmean.real
        
        # FrÃ©chetè·ç¦»å…¬å¼
        tr_covmean = np.trace(covmean)
        fad = diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean
        
        return float(fad)
    
    def compute_fad_from_audio(self, audio1, audio2, sr=16000):
        """
        ç›´æ¥ä»éŸ³é¢‘è®¡ç®—FAD
        æ³¨æ„ï¼šFADé€šå¸¸éœ€è¦å¤šä¸ªæ ·æœ¬ï¼Œè¿™é‡Œæ˜¯ç®€åŒ–ç‰ˆæœ¬
        """
        # æå–ç‰¹å¾
        features1 = self.extract_features(audio1, sr)
        features2 = self.extract_features(audio2, sr)
        
        # å¯¹äºå•ä¸ªæ ·æœ¬ï¼Œæˆ‘ä»¬ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ›å»º"ä¼ªæ‰¹æ¬¡"
        features1_batch = self._create_pseudo_batch(features1)
        features2_batch = self._create_pseudo_batch(features2)
        
        # è®¡ç®—FAD
        fad = self.calculate_fad(features1_batch, features2_batch)
        
        return fad
    
    def _create_pseudo_batch(self, features):
        """ä¸ºå•æ ·æœ¬åˆ›å»ºä¼ªæ‰¹æ¬¡ç”¨äºFADè®¡ç®—"""
        if len(features.shape) == 1:
            features = features.reshape(1, -1)
        
        # æ·»åŠ å™ªå£°åˆ›å»ºå˜ä½“
        batch = []
        for i in range(10):  # åˆ›å»º10ä¸ªè½»å¾®å˜ä½“
            noise = np.random.normal(0, 0.01, features.shape)
            batch.append(features + noise)
        
        return np.vstack(batch)


def compute_all_metrics_with_fad(original_audio, reconstructed_audio, sr=16000):
    """
    è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼ŒåŒ…æ‹¬FAD
    """
    print("\n=== è®¡ç®—å®Œæ•´è¯„ä¼°æŒ‡æ ‡ ===\n")
    
    results = {}
    
    # 1. æ—¶åŸŸæŒ‡æ ‡ï¼ˆå‚è€ƒç”¨ï¼‰
    print("è®¡ç®—æ—¶åŸŸæŒ‡æ ‡...")
    results['waveform_l1'] = np.mean(np.abs(original_audio - reconstructed_audio))
    results['waveform_l2'] = np.mean((original_audio - reconstructed_audio) ** 2)
    
    # 2. é¢‘åŸŸæŒ‡æ ‡ï¼ˆé‡è¦ï¼‰
    print("è®¡ç®—é¢‘åŸŸæŒ‡æ ‡...")
    
    # Multi-resolution STFT
    stft_losses = []
    for n_fft in [2048, 1024, 512, 256, 128]:
        hop_length = n_fft // 4
        
        # è®¡ç®—STFT
        orig_stft = np.abs(librosa.stft(original_audio, n_fft=n_fft, hop_length=hop_length))
        rec_stft = np.abs(librosa.stft(reconstructed_audio, n_fft=n_fft, hop_length=hop_length))
        
        # Spectral convergence
        spec_conv = np.linalg.norm(orig_stft - rec_stft, 'fro') / (np.linalg.norm(orig_stft, 'fro') + 1e-8)
        
        # Log magnitude loss  
        log_orig = np.log(orig_stft + 1e-8)
        log_rec = np.log(rec_stft + 1e-8)
        log_loss = np.mean(np.abs(log_orig - log_rec))
        
        stft_losses.append(spec_conv + log_loss)
    
    results['multi_resolution_stft'] = np.mean(stft_losses)
    results['spectral_convergence'] = spec_conv  # ä½¿ç”¨æœ€é«˜åˆ†è¾¨ç‡çš„
    results['log_magnitude_loss'] = log_loss
    
    # 3. Melé¢‘è°±å›¾æŸå¤±
    print("è®¡ç®—Melé¢‘è°±å›¾æŸå¤±...")
    mel_orig = librosa.feature.melspectrogram(y=original_audio, sr=sr, n_mels=80)
    mel_rec = librosa.feature.melspectrogram(y=reconstructed_audio, sr=sr, n_mels=80)
    results['mel_l1_loss'] = np.mean(np.abs(mel_orig - mel_rec))
    results['mel_l2_loss'] = np.mean((mel_orig - mel_rec) ** 2)
    
    # 4. FAD (æœ€é‡è¦ï¼)
    print("è®¡ç®—FAD...")
    try:
        fad_calculator = FADCalculator(model_name='vggish', device='cuda' if torch.cuda.is_available() else 'cpu')
        fad_score = fad_calculator.compute_fad_from_audio(original_audio, reconstructed_audio, sr)
        results['fad'] = fad_score
        
        # å‚è€ƒå€¼è§£é‡Š
        fad_quality = "ä¼˜ç§€" if fad_score < 2.0 else "è‰¯å¥½" if fad_score < 5.0 else "ä¸€èˆ¬" if fad_score < 10.0 else "è¾ƒå·®"
        results['fad_quality'] = fad_quality
    except Exception as e:
        print(f"FADè®¡ç®—å¤±è´¥: {e}")
        results['fad'] = None
    
    # 5. SI-SDR (å¯é€‰)
    print("è®¡ç®—SI-SDR...")
    try:
        from mir_eval.separation import bss_eval_sources
        sdr, sir, sar, _ = bss_eval_sources(original_audio.reshape(1, -1), reconstructed_audio.reshape(1, -1))
        results['si_sdr'] = sdr[0]
    except:
        # ç®€å•å®ç°
        alpha = np.dot(reconstructed_audio, original_audio) / (np.dot(original_audio, original_audio) + 1e-8)
        results['si_sdr'] = 20 * np.log10(np.linalg.norm(alpha * original_audio) / (np.linalg.norm(alpha * original_audio - reconstructed_audio) + 1e-8))
    
    return results


def print_evaluation_results(results):
    """æ‰“å°è¯„ä¼°ç»“æœçš„æ ¼å¼åŒ–è¾“å‡º"""
    print("\n" + "="*60)
    print(" "*20 + "VAEé‡å»ºè¯„ä¼°æŠ¥å‘Š")
    print("="*60)
    
    print("\nğŸ“Š ä¸»è¦æŒ‡æ ‡ (Primary Metrics):")
    print("-"*40)
    
    if results.get('fad') is not None:
        print(f"FAD Score:                    {results['fad']:.2f} [{results.get('fad_quality', 'æœªçŸ¥')}]")
        print(f"  â”œâ”€ AudioLDM2 å‚è€ƒå€¼:        ~2.0")
        print(f"  â”œâ”€ Stable Audio å‚è€ƒå€¼:     ~2.8")
        print(f"  â””â”€ è¯„ä»·: <2=ä¼˜ç§€, <5=è‰¯å¥½, <10=ä¸€èˆ¬")
    
    print(f"\nMulti-Resolution STFT Loss:  {results['multi_resolution_stft']:.4f}")
    print(f"  â””â”€ æ­£å¸¸èŒƒå›´: 2.0-4.0")
    
    print(f"\nSpectral Convergence:         {results['spectral_convergence']:.4f}")
    print(f"  â””â”€ æ­£å¸¸èŒƒå›´: 0.5-1.0")
    
    print(f"\nMel L1 Loss:                  {results['mel_l1_loss']:.4f}")
    print(f"  â””â”€ AudioLDM2èŒƒå›´: 0.05-0.15")
    
    print("\nğŸ“ˆ æ¬¡è¦æŒ‡æ ‡ (Secondary Metrics):")
    print("-"*40)
    
    print(f"Log Magnitude Loss:           {results['log_magnitude_loss']:.4f}")
    print(f"SI-SDR:                       {results.get('si_sdr', 0):.2f} dB")
    print(f"Mel L2 Loss:                  {results['mel_l2_loss']:.6f}")
    
    print("\nâš ï¸ ä»…ä¾›å‚è€ƒ (æ—¶åŸŸæŒ‡æ ‡ä¸é€‚ç”¨äºVAE):")
    print("-"*40)
    print(f"Waveform L1:                  {results['waveform_l1']:.4f}")
    print(f"Waveform L2:                  {results['waveform_l2']:.6f}")
    
    print("\n" + "="*60)
    print("ç»“è®º: ", end="")
    
    # è‡ªåŠ¨åˆ¤æ–­è´¨é‡
    if results.get('fad') is not None and results['fad'] < 5.0:
        if results['multi_resolution_stft'] < 4.0:
            print("âœ… VAEé‡å»ºè´¨é‡è‰¯å¥½ï¼Œç¬¦åˆé¢„æœŸï¼")
        else:
            print("âš ï¸ FADè‰¯å¥½ä½†STFTåé«˜ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥")
    elif results['multi_resolution_stft'] < 4.0:
        print("âœ… STFTæŸå¤±æ­£å¸¸ï¼ŒVAEå·¥ä½œæ­£å¸¸")
    else:
        print("âš ï¸ æŒ‡æ ‡åé«˜ï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–")
    
    print("="*60 + "\n")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å‡è®¾ä½ å·²ç»æœ‰äº†éŸ³é¢‘æ•°æ®
    import librosa
    
    # åŠ è½½éŸ³é¢‘
    original, sr = librosa.load("original.wav", sr=16000)
    reconstructed, sr = librosa.load("reconstructed.wav", sr=16000)
    
    # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
    results = compute_all_metrics_with_fad(original, reconstructed, sr)
    
    # æ‰“å°æ ¼å¼åŒ–ç»“æœ
    print_evaluation_results(results)
    
    # ä¹Ÿå¯ä»¥ä¿å­˜ä¸ºJSON
    import json
    with open("evaluation_results.json", "w") as f:
        json.dump(results, f, indent=2)
