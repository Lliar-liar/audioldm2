import torch
import torch.multiprocessing as mp
from torch.utils.data import Dataset, DataLoader
from diffusers import AudioLDM2Pipeline
import torchaudio
import os
import numpy as np
from tqdm import tqdm
import traceback
from audioldm2.utils import default_audioldm_config
from audioldm2.utilities.audio.stft import TacotronSTFT

# =====================================================================================
#  å¯è°ƒä¼˜çš„æ€§èƒ½å‚æ•°
# =====================================================================================
# BATCH_SIZE: ä¸€æ¬¡æ€§é€å…¥GPUå¤„ç†çš„æ–‡ä»¶æ•°é‡ã€‚æ ¹æ®ä½ çš„GPUæ˜¾å­˜è°ƒæ•´ã€‚
#             å¯¹äº3ç§’éŸ³é¢‘ï¼Œ32æˆ–64æ˜¯æ¯”è¾ƒåˆç†çš„å€¼ã€‚æ˜¾å­˜è¶Šå¤§ï¼Œå¯ä»¥è®¾ç½®å¾—è¶Šé«˜ã€‚
BATCH_SIZE = 32

# NUM_WORKERS: åœ¨åå°åŠ è½½å’Œé¢„å¤„ç†æ•°æ®çš„å­è¿›ç¨‹æ•°é‡ã€‚
#              å»ºè®®è®¾ç½®ä¸ºä½ æœåŠ¡å™¨CPUæ ¸å¿ƒæ•°çš„ä¸€åŠå·¦å³ï¼Œä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰16æ ¸ï¼Œå¯ä»¥è®¾ä¸º8ã€‚
#              è®¾ç½®ä¸º0è¡¨ç¤ºåªä½¿ç”¨ä¸»è¿›ç¨‹åŠ è½½æ•°æ®ï¼ˆä¼šå˜æ…¢ï¼‰ã€‚
NUM_WORKERS = 32
# =====================================================================================


# =====================================================================================
#  1. éŸ³é¢‘é¢„å¤„ç†çš„è¾…åŠ©å‡½æ•° (åŸºæœ¬ä¿æŒä¸å˜ï¼Œä½†å¢å¼ºäº†å¥å£®æ€§)
# =====================================================================================

def get_mel_from_wav(audio, _stft):
    audio = torch.clip(torch.FloatTensor(audio).unsqueeze(0), -1, 1)
    audio = torch.autograd.Variable(audio, requires_grad=False)
    melspec, magnitudes, phases, energy = _stft.mel_spectrogram(audio)
    melspec = torch.squeeze(melspec, 0).numpy().astype(np.float32)
    return melspec

def pad_wav(waveform, segment_length):
    waveform_length = waveform.shape[-1]
    if waveform_length > segment_length:
        return waveform[:, :segment_length]
    elif waveform_length < segment_length:
        temp_wav = np.zeros((1, segment_length))
        temp_wav[:, :waveform_length] = waveform
        return temp_wav
    return waveform

def _pad_spec(fbank, target_length=1024):
    n_frames = fbank.shape[0]
    p = target_length - n_frames
    if p > 0:
        m = torch.nn.ZeroPad2d((0, 0, 0, p))
        fbank = m(fbank)
    elif p < 0:
        fbank = fbank[0:target_length, :]
    return fbank

def normalize_wav(waveform):
    if np.max(np.abs(waveform)) < 1e-6:
        return waveform
    waveform = waveform - np.mean(waveform)
    waveform = waveform / (np.max(np.abs(waveform)) + 1e-8)
    return waveform * 0.5

def read_wav_file(filename, segment_length):
    try:
        waveform, sr = torchaudio.load(filename, format="mp4", backend="ffmpeg")
        if waveform.numel() == 0: # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºéŸ³é¢‘
            raise ValueError("Loaded waveform is empty.")
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=16000)
        waveform = waveform.numpy()
        waveform = normalize_wav(waveform)
        waveform = pad_wav(waveform, segment_length)
    except Exception as e:
        # tqdm.write(f"Warning: Failed to load {os.path.basename(filename)}. Error: {e}. Returning silence.")
        waveform = np.zeros((1, segment_length))
    return waveform

def wav_to_fbank(filename, target_length, fn_STFT):
    hop_size = 160  # From default config
    waveform = read_wav_file(filename, target_length * hop_size)
    waveform = waveform[0, ...]
    fbank = get_mel_from_wav(waveform, fn_STFT)
    fbank = torch.FloatTensor(fbank.T)
    fbank = _pad_spec(fbank, target_length)
    return fbank

# =====================================================================================
#  2. ä¼˜åŒ–çš„æ ¸å¿ƒï¼šè‡ªå®šä¹‰ Dataset
# =====================================================================================

class VideoAudioDataset(Dataset):
    """
    è¿™ä¸ªç±»è´Ÿè´£é«˜æ•ˆåœ°åŠ è½½å’Œé¢„å¤„ç†å•ä¸ªéŸ³é¢‘æ–‡ä»¶ã€‚
    DataLoaderå°†ä½¿ç”¨è¿™ä¸ªç±»ï¼Œå¹¶åˆ©ç”¨å¤šè¿›ç¨‹ï¼ˆnum_workersï¼‰æ¥å¹¶è¡Œæ‰§è¡Œè¿™é‡Œçš„æ“ä½œã€‚
    """
    def __init__(self, file_list, input_dir, duration=3):
        self.input_dir = input_dir
        self.file_list = file_list
        self.target_length = int(duration * 102.4)

        config = default_audioldm_config()
        self.fn_STFT = TacotronSTFT(
            config["preprocessing"]["stft"]["filter_length"],
            config["preprocessing"]["stft"]["hop_length"],
            config["preprocessing"]["stft"]["win_length"],
            config["preprocessing"]["mel"]["n_mel_channels"],
            config["preprocessing"]["audio"]["sampling_rate"],
            config["preprocessing"]["mel"]["mel_fmin"],
            config["preprocessing"]["mel"]["mel_fmax"],
        )

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        filename = self.file_list[idx]
        video_path = os.path.join(self.input_dir, filename)
        
        try:
            mel = wav_to_fbank(video_path, self.target_length, self.fn_STFT)
        except Exception:
            # å¦‚æœæŸä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªå…¨é›¶çš„å¼ é‡ï¼Œä»¥ä¿è¯æ‰¹æ¬¡å¤„ç†ä¸ä¸­æ–­
            mel = torch.zeros((self.target_length, 80)) # 80 is n_mel_channels
        
        return mel, video_path

# =====================================================================================
#  3. æ¨¡å‹åŠ è½½ä¸ä¼˜åŒ–çš„å·¥ä½œè¿›ç¨‹å‡½æ•°
# =====================================================================================

def setup_audioldm2_vae(gpu_id, repo_id="cvssp/audioldm2", torch_dtype=torch.float16):
    device = f"cuda:{gpu_id}"
    print(f"[GPU-{gpu_id}]: æ­£åœ¨åŠ è½½ AudioLDM 2 VAE åˆ° {device}...")
    pipe = AudioLDM2Pipeline.from_pretrained(repo_id, torch_dtype=torch_dtype, resume_download=True)
    pipe = pipe.to(device)
    print(f"[GPU-{gpu_id}]: æ¨¡å‹å·²æˆåŠŸåŠ è½½ã€‚")
    return pipe.vae, device

def process_files_on_gpu_optimized(gpu_id, file_chunk, input_dir, output_dir):
    """
    è¿™æ˜¯ç”±å•ä¸ªGPUè¿›ç¨‹æ‰§è¡Œçš„å·¥ä½œå‡½æ•°ã€‚
    å®ƒå†…éƒ¨ä½¿ç”¨DataLoaderæ¥åˆ›å»ºæ•°æ®åŠ è½½æµæ°´çº¿ã€‚
    """
    try:
        vae, device = setup_audioldm2_vae(gpu_id)
    except Exception as e:
        print(f"[GPU-{gpu_id}]: æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return len(file_chunk), 0

    dataset = VideoAudioDataset(file_chunk, input_dir)
    data_loader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS,
        pin_memory=True,
        shuffle=False
    )
    
    success_count = 0
    error_count = 0
    
    progress_bar = tqdm(data_loader, desc=f"GPU-{gpu_id} å¤„ç†ä¸­", position=gpu_id, leave=True)
    
    for mel_batch, path_batch in progress_bar:
        try:
            final_mel_batch = []
            final_output_paths = []

            # è¿‡æ»¤æ‰å·²ç»å­˜åœ¨çš„æ–‡ä»¶ï¼Œé¿å…é‡å¤è®¡ç®—
            for i, video_path in enumerate(path_batch):
                base_name = os.path.splitext(os.path.basename(video_path))[0]
                output_path = os.path.join(output_dir, f"{base_name}.npy")
                if not os.path.exists(output_path):
                    final_mel_batch.append(mel_batch[i])
                    final_output_paths.append(output_path)
            
            if not final_mel_batch:
                success_count += len(path_batch)
                continue

            # å°†éœ€è¦å¤„ç†çš„æ•°æ®åˆå¹¶æˆä¸€ä¸ªæ–°çš„æ‰¹æ¬¡
            mel_to_process = torch.stack(final_mel_batch)
            mel_to_process = mel_to_process.unsqueeze(1).to(device, dtype=torch.float16)
            
            with torch.no_grad():
                latent_batch = vae.encode(mel_to_process).latent_dist.mode()
            
            latent_batch_np = latent_batch.cpu().numpy()

            for latent_np, output_path in zip(latent_batch_np, final_output_paths):
                np.save(output_path, latent_np)
            
            success_count += len(path_batch) # å°†è·³è¿‡å’Œå·²å¤„ç†çš„éƒ½ç®—ä½œæˆåŠŸ

        except Exception as e:
            tqdm.write(f"[GPU-{gpu_id} é”™è¯¯]: æ‰¹å¤„ç†å¤±è´¥ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡ã€‚é”™è¯¯: {e}")
            error_count += len(path_batch)

    return error_count, success_count

# =====================================================================================
#  4. ä¸»å‡½æ•°å’Œä»»åŠ¡åˆ†å‘
# =====================================================================================

def batch_process_videos_multi_gpu(input_dir, output_dir):
    if not torch.cuda.is_available():
        print("é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°CUDAè®¾å¤‡ã€‚")
        return

    num_gpus = torch.cuda.device_count()
    print(f"æ£€æµ‹åˆ° {num_gpus} å—å¯ç”¨çš„GPUã€‚")

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"å·²åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")

    all_files = sorted([f for f in os.listdir(input_dir) if f.lower().endswith((".mp4", ".wav", ".flac", ".m4a"))])
    if not all_files:
        print(f"åœ¨ç›®å½• '{input_dir}' ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„éŸ³/è§†é¢‘æ–‡ä»¶ã€‚")
        return
        
    print(f"åœ¨è¾“å…¥ç›®å½•ä¸­æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶ã€‚å‡†å¤‡åˆ†å‘ä»»åŠ¡...")

    file_chunks = np.array_split(all_files, num_gpus)
    
    tasks = []
    for gpu_id, chunk in enumerate(file_chunks):
        if len(chunk) > 0:
            tasks.append((gpu_id, list(chunk), input_dir, output_dir))
            print(f"  -> GPU-{gpu_id} å°†å¤„ç† {len(chunk)} ä¸ªæ–‡ä»¶ã€‚")

    ctx = mp.get_context('spawn')
    with ctx.Pool(processes=len(tasks)) as pool:
        results = pool.starmap(process_files_on_gpu_optimized, tasks)

    total_errors = sum([res[0] for res in results])
    total_success = sum([res[1] for res in results])

    print("\n" + "="*50)
    print("--- å¤„ç†å®Œæˆï¼Œç”ŸæˆæŠ¥å‘Š ---")
    print(f"æ€»è®¡æˆåŠŸå¤„ç†: {total_success} ä¸ªæ–‡ä»¶")
    print(f"æ€»è®¡å¤„ç†å¤±è´¥: {total_errors} ä¸ªæ–‡ä»¶")
    print("="*50 + "\n")


if __name__ == '__main__':
    try:
        mp.set_start_method('spawn', force=True)
        print("å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•å·²è®¾ç½®ä¸º 'spawn'ã€‚")
    except RuntimeError:
        print("å¯åŠ¨æ–¹æ³•å·²ç»è¢«è®¾ç½®ï¼Œå¿½ç•¥ã€‚")

    # =======================================================================
    #  åœ¨è¿™é‡Œé…ç½®ä½ çš„è¾“å…¥è¾“å‡ºç›®å½•
    # =======================================================================
    video_directory_list = [
        "vggsound_00_3s", "vggsound_01_3s", "vggsound_02_3s", "vggsound_03_3s", 
        "vggsound_04_3s", "vggsound_05_3s", "vggsound_06_3s", "vggsound_07_3s", 
        "vggsound_08_3s", "vggsound_09_3s", "vggsound_10_3s", "vggsound_11_3s", 
        "vggsound_12_3s", "vggsound_13_3s", "vggsound_14_3s"
    ]
    input_video_directory_base = "/blob/vggsound_cropped"
    output_latent_directory_base = "/blob/vggsound_cropped_audio_latent"
    # =======================================================================

    for video_dir in video_directory_list:
        input_video_directory = os.path.join(input_video_directory_base, video_dir)
        output_latent_directory = os.path.join(output_latent_directory_base, video_dir)

        print(f"\n{'='*20} å¼€å§‹å¤„ç†ç›®å½•: {video_dir} {'='*20}")
        print(f"è¾“å…¥: {input_video_directory}")
        print(f"è¾“å‡º: {output_latent_directory}")

        if not os.path.isdir(input_video_directory):
            print(f"è­¦å‘Š: è¾“å…¥ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡: {input_video_directory}")
            continue

        try:
            batch_process_videos_multi_gpu(input_video_directory, output_latent_directory)
        except Exception as e:
            print(f"\nå¤„ç†ç›®å½• '{video_dir}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            print(traceback.format_exc())

    print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆï¼")